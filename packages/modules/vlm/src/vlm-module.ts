/**
 * VLM Module implementation for browser-ai v2.0
 * Local Vision-Language Model for image understanding
 * CDC v2026.9 ยง12
 * 
 * Uses Transformers.js with WebGPU backend for:
 * - Image captioning (BLIP/GIT models)
 * - Visual question answering
 * - Image-to-text chat
 * 
 * Tier 3 only (high-end devices with WebGPU)
 */

import { BrowserAIError, BrowserAIErrorCode, getGlobalRegistry, type UnifiedModelRegistry } from '@cohesiumai/core';
import type {
  VlmConfig,
  VlmResult,
  VlmModule,
  VlmModuleState,
  VlmDiagnostics,
  VlmBackend,
  DeviceTier,
} from './types.js';

// Types for dynamic import
type ImageToTextPipeline = (
  image: string | Blob,
  options?: { max_new_tokens?: number }
) => Promise<Array<{ generated_text: string }>>;

type VqaPipeline = (
  image: string | Blob,
  question: string,
  options?: { top_k?: number }
) => Promise<Array<{ answer: string; score: number }>>;

/**
 * Detect device tier based on hardware capabilities
 * CDC v2026.9 ยง13
 */
function detectTier(): DeviceTier {
  const nav = navigator as { hardwareConcurrency?: number };
  const cores = nav.hardwareConcurrency || 4;
  const ua = navigator.userAgent || '';
  const mobile = /Mobi|Android/i.test(ua);

  if (mobile) return 1;
  if (cores >= 8) return 3;
  return 2;
}

function createVlmError(
  code: BrowserAIErrorCode,
  message: string,
  cause?: unknown
): BrowserAIError {
  return {
    code,
    message,
    recoverability: 'non-recoverable',
    cause: cause instanceof Error ? cause : undefined,
    userAction: 'VLM requires a high-performance device (tier 3). Consider using OCR module instead.',
    devAction: 'Check device tier with detectTier() before initializing VLM.',
    timestampMs: Date.now(),
  };
}

class VlmModuleImpl implements VlmModule {
  private initialized = false;
  private backend: VlmBackend = 'webgpu';
  private deviceTier: DeviceTier = 2;
  private modelLoaded = false;
  private lastLatencyMs = 0;
  private imagesProcessed = 0;
  private captioner: ImageToTextPipeline | null = null;
  private vqa: VqaPipeline | null = null;
  private registry: UnifiedModelRegistry = getGlobalRegistry();
  private captionerModelId: string | null = null;

  async init(cfg: VlmConfig): Promise<void> {
    if (cfg.privacyMode !== 'fully-local-managed') {
      throw createVlmError(
        'ERROR_VLM_INIT_FAILED',
        'VLM module requires privacyMode: fully-local-managed'
      );
    }

    this.deviceTier = detectTier();

    // VLM requires tier 3 by default
    const requireTier3 = cfg.requireTier3 ?? true;
    if (requireTier3 && this.deviceTier < 3) {
      throw createVlmError(
        'ERROR_VLM_TIER_NOT_SUPPORTED',
        `VLM requires tier 3 device. Current tier: ${this.deviceTier}. ` +
        `Consider using @cohesiumai/modules-ocr for image text extraction on lower-tier devices.`
      );
    }

    // Check WebGPU availability
    if (!this.isWebGPUAvailable()) {
      throw createVlmError(
        'ERROR_VLM_INIT_FAILED',
        'WebGPU is not available in this browser. VLM requires WebGPU support.'
      );
    }

    try {
      const modelId = 'Xenova/vit-gpt2-image-captioning';
      this.captionerModelId = modelId;
      
      // Use registry for shared model management
      this.captioner = await this.registry.acquire(
        modelId,
        'transformers',
        async () => {
          console.log('[VLM] Loading vision-language model via registry...');
          const transformers = await import('@huggingface/transformers');
          return transformers.pipeline(
            'image-to-text',
            modelId,
            { device: 'webgpu' }
          );
        },
        { sizeEstimateMB: 350 }
      ) as unknown as ImageToTextPipeline;
      
      // VQA is handled by the captioner with prompt context
      this.vqa = null;
      
      this.modelLoaded = true;
      this.initialized = true;
      console.log('[VLM] Models loaded successfully via registry');
    } catch (err) {
      console.error('[VLM] Model loading failed:', err);
      this.captionerModelId = null;
      throw createVlmError(
        'ERROR_VLM_INIT_FAILED',
        `Failed to load VLM models: ${err instanceof Error ? err.message : String(err)}`,
        err
      );
    }
  }

  private isWebGPUAvailable(): boolean {
    // Check if navigator.gpu exists
    const nav = navigator as { gpu?: unknown };
    return !!nav.gpu;
  }

  async describeImage(image: Blob | ArrayBuffer): Promise<VlmResult> {
    this.assertInitialized();

    if (!this.captioner) {
      throw createVlmError(
        'ERROR_VLM_INIT_FAILED',
        'Captioner model not loaded'
      );
    }

    const startTime = performance.now();

    try {
      // Convert ArrayBuffer to Blob if needed
      const imageBlob = image instanceof Blob 
        ? image 
        : new Blob([image], { type: 'image/png' });
      
      // Create object URL for the image
      const imageUrl = URL.createObjectURL(imageBlob);
      
      try {
        // Run image captioning
        const outputs = await this.captioner(imageUrl, { max_new_tokens: 100 });
        
        const text = outputs[0]?.generated_text ?? 'No description generated';
        
        const result: VlmResult = {
          text,
          confidence: 0.85,
          durationMs: performance.now() - startTime,
        };

        this.lastLatencyMs = result.durationMs || 0;
        this.imagesProcessed += 1;
        
        console.log(`[VLM] Image described in ${Math.round(this.lastLatencyMs)}ms`);
        return result;
      } finally {
        URL.revokeObjectURL(imageUrl);
      }
    } catch (err) {
      throw createVlmError(
        'ERROR_VLM_INIT_FAILED',
        `Image description failed: ${err instanceof Error ? err.message : String(err)}`,
        err
      );
    }
  }

  async chatWithImage(input: { image: Blob | ArrayBuffer; prompt: string }): Promise<VlmResult> {
    this.assertInitialized();

    const startTime = performance.now();

    try {
      // Convert ArrayBuffer to Blob if needed
      const imageBlob = input.image instanceof Blob 
        ? input.image 
        : new Blob([input.image], { type: 'image/png' });
      
      // Create object URL for the image
      const imageUrl = URL.createObjectURL(imageBlob);
      
      try {
        let text: string;
        
        // Try VQA if available, otherwise use captioner with prompt context
        if (this.vqa) {
          try {
            const outputs = await this.vqa(imageUrl, input.prompt, { top_k: 1 });
            text = outputs[0]?.answer ?? 'No answer generated';
          } catch {
            // Fallback to captioner
            const outputs = await this.captioner!(imageUrl, { max_new_tokens: 100 });
            text = `Based on the image: ${outputs[0]?.generated_text ?? 'No description'}`;
          }
        } else if (this.captioner) {
          const outputs = await this.captioner(imageUrl, { max_new_tokens: 100 });
          text = `Based on the image: ${outputs[0]?.generated_text ?? 'No description'}`;
        } else {
          throw new Error('No VLM model available');
        }
        
        const result: VlmResult = {
          text,
          confidence: 0.8,
          durationMs: performance.now() - startTime,
        };

        this.lastLatencyMs = result.durationMs || 0;
        this.imagesProcessed += 1;
        
        console.log(`[VLM] Chat completed in ${Math.round(this.lastLatencyMs)}ms`);
        return result;
      } finally {
        URL.revokeObjectURL(imageUrl);
      }
    } catch (err) {
      throw createVlmError(
        'ERROR_VLM_INIT_FAILED',
        `VLM chat failed: ${err instanceof Error ? err.message : String(err)}`,
        err
      );
    }
  }

  getState(): VlmModuleState {
    return {
      initialized: this.initialized,
      backend: this.backend,
      deviceTier: this.deviceTier,
      modelLoaded: this.modelLoaded,
    };
  }

  getDiagnostics(): VlmDiagnostics {
    return {
      enabled: this.initialized,
      backend: this.backend,
      deviceTier: this.deviceTier,
      lastLatencyMs: this.lastLatencyMs,
      imagesProcessed: this.imagesProcessed,
    };
  }

  async teardown(): Promise<void> {
    // Release captioner from registry
    if (this.captionerModelId) {
      this.registry.release(this.captionerModelId);
      this.captionerModelId = null;
    }
    
    // Clear local references
    this.captioner = null;
    this.vqa = null;
    this.initialized = false;
    this.modelLoaded = false;
    this.lastLatencyMs = 0;
    this.imagesProcessed = 0;
    
    console.log('[VLM] Teardown complete (models released to registry)');
  }

  private assertInitialized(): void {
    if (!this.initialized) {
      throw createVlmError(
        'ERROR_VLM_INIT_FAILED',
        'VLM module not initialized. Call init() first.'
      );
    }
  }
}

export function createVlmModule(): VlmModule {
  return new VlmModuleImpl();
}

export { detectTier };

/**
 * Check if VLM is supported on current device.
 * Use this to determine whether to use VLM or fall back to OCR.
 * CDC v2026.9 ยง12.2
 */
export function isVlmSupported(): { supported: boolean; tier: DeviceTier; reason?: string } {
  const tier = detectTier();
  
  if (tier < 3) {
    return {
      supported: false,
      tier,
      reason: `Device tier ${tier} is below the required tier 3. Consider using @cohesiumai/modules-ocr instead.`,
    };
  }
  
  const nav = navigator as { gpu?: unknown };
  if (!nav.gpu) {
    return {
      supported: false,
      tier,
      reason: 'WebGPU is not available. VLM requires WebGPU support.',
    };
  }
  
  return { supported: true, tier };
}

/**
 * Create VLM or return null if not supported.
 * Useful for conditional initialization with OCR fallback.
 * CDC v2026.9 ยง12.2
 * 
 * @example
 * const vlm = tryCreateVlmModule();
 * if (!vlm) {
 *   // Fall back to OCR
 *   const ocr = createOcrModule();
 * }
 */
export function tryCreateVlmModule(): VlmModule | null {
  const check = isVlmSupported();
  if (!check.supported) {
    console.warn(`[VLM] Not supported: ${check.reason}`);
    return null;
  }
  return new VlmModuleImpl();
}
