# browser-ai

> **"The jQuery of local AI"** â€” A unified API that abstracts browser AI runtime fragmentation (Native / WebNN / WebGPU / WASM).

[![npm version](https://img.shields.io/npm/v/@browser-ai/core.svg)](https://www.npmjs.com/package/@browser-ai/core)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-blue.svg)](https://www.typescriptlang.org/)

## Why browser-ai?

Running AI models directly in the browser offers **privacy**, **offline capability**, and **zero server costs**. However, the ecosystem is fragmented:

- **Chrome AI (Prompt API)** â€” Native, fast, but Chrome-only
- **WebLLM** â€” WebGPU-based, cross-browser, but requires model download
- **WebNN** â€” Hardware-accelerated, but limited browser support
- **WASM** â€” Universal fallback, but slower

**browser-ai** provides a single API that automatically selects the best available runtime.

## Features

- ðŸš€ **Zero Config** â€” Native AI when available, WebLLM fallback automatically
- ðŸ”’ **Privacy-First** â€” All inference runs 100% locally in your browser
- âš¡ **Production-Ready** â€” FSM-based state management, typed errors, diagnostics
- ðŸ§ª **CI-Friendly** â€” MockProvider for GPU-free testing
- ðŸ“¦ **Modular** â€” Tree-shakeable, import only what you need
- ðŸŽ¯ **TypeScript** â€” Full type safety with strict mode

## Installation

```bash
# npm
npm install @browser-ai/core @browser-ai/react @browser-ai/providers-webllm

# pnpm
pnpm add @browser-ai/core @browser-ai/react @browser-ai/providers-webllm

# yarn
yarn add @browser-ai/core @browser-ai/react @browser-ai/providers-webllm
```

## Quick Start

### React

```tsx
import { useLocalCompletion } from '@browser-ai/react';
import { createNativeProvider } from '@browser-ai/providers-native';
import { createWebLLMProvider } from '@browser-ai/providers-webllm';

const config = {
  privacyMode: 'strict', // 'strict' | 'relaxed' | 'any'
  providerPolicy: { order: ['native', 'webllm'] },
};

const providers = [createNativeProvider(), createWebLLMProvider()];

function App() {
  const { state, output, generate, abort, error } = useLocalCompletion({
    config,
    providers,
    autoInit: true,
  });

  const handleGenerate = () => {
    generate({
      messages: [{ role: 'user', content: 'Explain quantum computing in 3 sentences.' }],
      maxTokens: 150,
      temperature: 0.7,
    });
  };

  return (
    <div>
      <p>Status: {state.name}</p>
      {error && <p style={{ color: 'red' }}>Error: {error.message}</p>}
      <button onClick={handleGenerate} disabled={state.name !== 'READY'}>
        Generate
      </button>
      {state.name === 'GENERATING' && <button onClick={abort}>Abort</button>}
      <pre>{output}</pre>
    </div>
  );
}
```

### Vanilla JavaScript

```typescript
import { createBrowserAI } from '@browser-ai/core';
import { createWebLLMProvider } from '@browser-ai/providers-webllm';

const ai = createBrowserAI({
  config: { providerPolicy: { order: ['webllm'] } },
  providers: [createWebLLMProvider()],
});

await ai.init();

const { result } = ai.generate({
  messages: [{ role: 'user', content: 'Hello!' }],
});

const response = await result;
console.log(response.text);
```

## Packages

### Core & Providers

| Package | Description | Size |
|---------|-------------|------|
| `@browser-ai/core` | Core types, FSM, protocol, diagnostics, plugins | ~50KB |
| `@browser-ai/providers-native` | Chrome AI (Prompt API) provider | ~3KB |
| `@browser-ai/providers-webllm` | WebGPU/WebLLM provider | ~5KB + runtime |
| `@browser-ai/providers-webnn` | WebNN hardware-accelerated provider | ~3KB |
| `@browser-ai/providers-wasm` | WASM universal fallback provider | ~3KB |
| `@browser-ai/providers-mock` | Mock provider for CI/testing | ~2KB |
| `@browser-ai/react` | React hooks (`useLocalCompletion`) | ~4KB |
| `@browser-ai/ui` | Pre-built UI components | ~8KB |

### Modules (v1.1+)

| Package | Version | Description |
|---------|---------|-------------|
| `@browser-ai/modules-audio` | v1.1 | Local ASR, VAD, TTS (speech-to-text, voice detection, text-to-speech) |
| `@browser-ai/modules-ocr` | v1.2 | Local OCR for images & PDFs with text extraction pipeline |
| `@browser-ai/modules-memory` | v1.3 | Conversation context & local memory with IndexedDB |
| `@browser-ai/modules-vlm` | v2.0 | Vision-Language Model for image understanding (tier 3 only) |

## Browser Support

| Browser | Native AI | WebLLM | WebNN | WASM | Status |
|---------|-----------|--------|-------|------|--------|
| Chrome 127+ | âœ… | âœ… | âœ… | âœ… | Full support |
| Chrome <127 | âŒ | âœ… | âš ï¸ | âœ… | WebLLM/WASM |
| Edge 127+ | âœ… | âœ… | âœ… | âœ… | Full support |
| Firefox | âŒ | âœ… | âŒ | âœ… | WebLLM/WASM |
| Safari 18+ | âŒ | âš ï¸ | âŒ | âœ… | WASM fallback |

## Configuration

```typescript
interface BrowserAIConfig {
  // Privacy mode controls which providers are allowed
  privacyMode?: 'strict' | 'relaxed' | 'any';
  
  // Provider selection policy
  providerPolicy: {
    order: ('native' | 'webllm' | 'webnn' | 'wasm' | 'mock')[];
  };
  
  // Optional timeouts
  timeouts?: {
    timeoutMultiplier?: number; // Default: 1.0
  };
  
  // Force a specific provider tier (V0.2+)
  tierOverride?: 'native' | 'webllm' | 'webnn' | 'wasm';
}
```

## State Machine

browser-ai uses a finite state machine with 12 states:

```
IDLE â†’ BOOTING â†’ SELECTING_PROVIDER â†’ PREFLIGHT_QUOTA â†’ CHECKING_CACHE
  â†“
DOWNLOADING â†’ WARMING_UP â†’ READY â‡„ GENERATING
  â†“
ERROR (recoverable) â†’ READY
  â†“
TEARING_DOWN â†’ IDLE
```

## Diagnostics

Get detailed runtime information:

```typescript
const diagnostics = ai.getDiagnostics();
// or
const diagnostics = completion.getDiagnostics();

console.log(diagnostics);
// {
//   schemaVersion: '1',
//   state: { name: 'READY', sinceMs: 1234567890 },
//   selectionReport: { selected: 'webllm', reasons: [...] },
//   capabilities: { hasWebGPU: true, hasWindowAI: false },
//   cache: { modelId: 'Llama-3.2-3B-Instruct-q4f16_1-MLC' },
//   timings: { bootMs: 2500 },
//   ...
// }
```

## Multi-Model Support (V1.0)

Load and switch between multiple models:

```typescript
import { createModelManager } from '@browser-ai/core';

const manager = createModelManager({
  maxLoadedModels: 2,  // Keep up to 2 models in memory
  autoUnload: true,    // Auto-evict LRU model when limit reached
});

// Load models
await manager.loadModel(smallModelSpec, provider1);
await manager.loadModel(largeModelSpec, provider2);

// Switch active model
await manager.setActiveModel('large-model-id');

// Get active model for inference
const active = manager.getActiveModel();
```

## Plugin System (V1.0)

Extend browser-ai with plugins:

```typescript
import { createPluginManager, createLoggingPlugin } from '@browser-ai/core';

const plugins = createPluginManager();

// Built-in logging plugin
plugins.register(createLoggingPlugin({ prefix: '[AI]' }));

// Custom plugin
plugins.register({
  name: 'analytics',
  afterGenerate(ctx) {
    trackEvent('ai_generate', { tokens: ctx.result.usage?.totalTokens });
  },
  onError(ctx) {
    reportError(ctx.error);
  },
});

// Available hooks:
// beforeInit, afterInit, beforeGenerate, afterGenerate,
// onToken, onStateChange, onError, beforeTeardown, afterTeardown
```

## OPFS Storage (V1.0)

Persistent model storage that survives cache clearing:

```typescript
import { createOPFSManager } from '@browser-ai/core';

const opfs = createOPFSManager();

if (opfs.isAvailable()) {
  // Store model shard
  await opfs.storeShard('model-id', 0, shardData);
  
  // Check storage info
  const info = await opfs.getStorageInfo();
  console.log(`Used: ${info.usedBytes} bytes, Models: ${info.models.length}`);
}
```

## LRU Cache (V1.0)

Automatic cache management:

```typescript
import { createLRUCacheManager } from '@browser-ai/core';

const cache = await createLRUCacheManager({
  maxUsageRatio: 0.8,      // Use up to 80% of quota
  minFreeBytes: 500_000_000, // Keep 500MB free
});

// Auto-evict old models when needed
const { evicted, freedBytes } = await cache.autoEvict();

// Get storage stats
const stats = await cache.getStats();
console.log(`${stats.modelCount} models, ${stats.usedBytes} bytes used`);
```

## Unified Model Registry (V2.1)

Central memory management for all AI models across core and modules. Solves the problem of fragmented model loading where each module (audio, memory, VLM) loads models independently.

```typescript
import { getGlobalRegistry } from '@browser-ai/core';

const registry = getGlobalRegistry({
  maxMemoryMB: 1500,           // Max total memory before LRU eviction
  defaultIdleTimeoutMs: 300000, // Auto-unload after 5min idle
});

// Acquire a model (loads if not cached, increments refCount)
const embedder = await registry.acquire(
  'Xenova/all-MiniLM-L6-v2',
  'transformers',
  async () => {
    const transformers = await import('@huggingface/transformers');
    return transformers.pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
  },
  { sizeEstimateMB: 90 }
);

// Release when done (decrements refCount, starts idle timer)
registry.release('Xenova/all-MiniLM-L6-v2');

// Check memory usage
const usage = registry.getMemoryUsage();
console.log(`Total: ${usage.totalMB}MB across ${usage.models.length} models`);

// Manual eviction if needed
const evicted = await registry.evictLRU(500); // Free 500MB
```

### Key Features

- **Reference Counting** â€” Models stay loaded while in use by any module
- **Auto-Teardown** â€” Automatic unload after configurable idle timeout
- **LRU Eviction** â€” Oldest unused models evicted when memory limit reached
- **Shared Instances** â€” Same model reused across audio, memory, VLM modules
- **Memory Tracking** â€” Real-time usage estimates per model

### How Modules Use It

All modules (`@browser-ai/modules-audio`, `@browser-ai/modules-memory`, `@browser-ai/modules-vlm`) automatically use the global registry:

```typescript
// Behind the scenes in audio module:
this.asrPipeline = await registry.acquire('Xenova/whisper-tiny', 'transformers', loader);

// On teardown:
registry.release('Xenova/whisper-tiny');
// Model stays in memory for 5min in case another module needs it
```

## Installation from npm

All packages are published under the `@browser-ai/*` scope on npm:

```bash
# Core + React + a provider
npm install @browser-ai/core @browser-ai/react @browser-ai/providers-webllm

# Or with pnpm
pnpm add @browser-ai/core @browser-ai/react @browser-ai/providers-webllm
```

### Available Packages

```bash
# Core
npm install @browser-ai/core

# Providers
npm install @browser-ai/providers-native
npm install @browser-ai/providers-webllm
npm install @browser-ai/providers-webnn
npm install @browser-ai/providers-wasm
npm install @browser-ai/providers-mock

# React bindings
npm install @browser-ai/react
npm install @browser-ai/ui

# Modules
npm install @browser-ai/modules-audio
npm install @browser-ai/modules-ocr
npm install @browser-ai/modules-memory
npm install @browser-ai/modules-vlm

# CLI
npm install -g @browser-ai/cli
```

## Development

```bash
# Clone the repository
git clone https://github.com/CohesiumAI/browser-ai.git
cd browser-ai

# Install dependencies
pnpm install

# Build all packages
pnpm build

# Run the Vite example
pnpm dev

# Run tests
pnpm test

# Type check
pnpm typecheck

# Full verification (lint + typecheck + test + build)
pnpm run verify
```

## Modules Usage

### Audio Module (v1.1)

```typescript
import { createAudioModule } from '@browser-ai/modules-audio';

const audio = createAudioModule();
await audio.init({
  privacyMode: 'fully-local-managed',
  asr: { enabled: true, model: 'default', language: 'en' },
  vad: { enabled: true, sensitivity: 0.5 },
  tts: { enabled: true, voice: 'neutral' },
});

// Speech-to-text
const transcript = await audio.transcribe(audioBlob);
console.log(transcript.text);

// Voice activity detection
const vad = await audio.detectVoiceActivity(audioBuffer);
console.log(vad.isSpeech, vad.confidence);

// Text-to-speech
const speech = await audio.synthesize('Hello world!');
// speech.audioBuffer contains PCM audio data
```

### OCR Module (v1.2)

```typescript
import { createOcrModule } from '@browser-ai/modules-ocr';

const ocr = createOcrModule();
await ocr.init({
  privacyMode: 'fully-local-managed',
  language: 'eng',
  pdf: { preferTextLayer: true },
});

// OCR an image
const imageResult = await ocr.ocrImage(imageBlob);
console.log(imageResult.text);

// OCR a PDF (extracts text layer first, falls back to OCR)
const pdfResult = await ocr.ocrPdf(pdfBlob);
console.log(pdfResult.pages);

// Extract chunks for RAG pipeline
const chunks = await ocr.extractChunks(pdfResult.text, {
  chunkSizeChars: 1000,
  overlapChars: 100,
});
```

### Memory Module (v1.3)

```typescript
import { createMemoryModule } from '@browser-ai/modules-memory';

const memory = createMemoryModule();
await memory.init({
  privacyMode: 'fully-local-managed',
  conversationId: 'chat-123',
  maxTurns: 20,
  summaryEveryTurns: 10,
});

// Add conversation turns
await memory.addTurn({ role: 'user', content: 'Hello!', createdAtMs: Date.now() });
await memory.addTurn({ role: 'assistant', content: 'Hi there!', createdAtMs: Date.now() });

// Get context for prompt injection
const context = await memory.getContext();
console.log(context.summary, context.recentTurns);
```

### VLM Module (v2.0)

```typescript
import { createVlmModule, detectTier } from '@browser-ai/modules-vlm';

// VLM requires tier 3 device (8+ CPU cores)
const tier = detectTier();
if (tier < 3) {
  console.warn('VLM works best on tier 3 devices');
}

const vlm = createVlmModule();
await vlm.init({
  privacyMode: 'fully-local-managed',
  requireTier3: true, // Set to false to allow on lower-tier devices
});

// Describe an image
const description = await vlm.describeImage(imageBlob);
console.log(description.text);

// Chat with image context
const response = await vlm.chatWithImage({
  image: imageBlob,
  prompt: 'What objects are in this image?',
});
console.log(response.text);
```

## Examples

- **[Vite Text](./examples/vite-text)** â€” Text generation example
- **[Vite Audio](./examples/vite-audio)** â€” Audio ASR/VAD/TTS demo
- **[Vite OCR](./examples/vite-ocr)** â€” Image & PDF OCR demo
- **[Vite Memory](./examples/vite-memory)** â€” Conversation memory demo
- **[Vite VLM](./examples/vite-vlm)** â€” Vision-Language Model demo
- **[Vite Full](./examples/vite-full)** â€” All modules combined
- **[Next.js](./examples/nextjs)** â€” Server-side rendering compatible

## Version History

### V2.1 â€” Current
- âœ… **Unified Model Registry** â€” Central memory management for all modules
- âœ… **Reference Counting** â€” Models stay loaded while in use
- âœ… **Auto-Teardown** â€” Automatic unload after idle timeout
- âœ… **LRU Eviction** â€” Memory-aware model eviction across modules
- âœ… **Abort Recovery Fix** â€” Watchdog timing reset after engine recreation
- âœ… **`onRecreate` Callback** â€” Provider-to-core communication for engine recreation
- âœ… **CI Stability** â€” Test suite improvements with proper mocks

### V2.0
- âœ… **VLM Module** â€” Local Vision-Language Model for image understanding (tier 3 only)
- âœ… **Full Example** â€” Demo page combining all modules

### V1.3
- âœ… **Memory Module** â€” Conversation context with IndexedDB storage
- âœ… **Local Summarization** â€” Auto-summarize long conversations

### V1.2
- âœ… **OCR Module** â€” Local image & PDF text extraction
- âœ… **Extractive Pipeline** â€” Chunk text for RAG workflows

### V1.1
- âœ… **Audio Module** â€” Local ASR (speech-to-text), VAD, TTS
- âœ… **Privacy-First Audio** â€” All processing 100% local

### V1.0
- âœ… **OPFS Storage** â€” Persistent model storage via Origin Private File System
- âœ… **LRU Cache Manager** â€” Automatic eviction when quota low
- âœ… **Multi-Model Support** â€” Load multiple models simultaneously
- âœ… **Plugin Architecture** â€” Extensible hooks system

### V0.2
- âœ… **WebNN Provider** â€” Hardware-accelerated inference
- âœ… **WASM Provider** â€” Universal fallback for all browsers
- âœ… **Healthcheck Token-Aware** â€” Smarter stall detection
- âœ… **Download Watchdog** â€” Stuck download detection
- âœ… **tierOverride Config** â€” Force specific provider

### V0.1
- âœ… Native provider (Chrome AI Prompt API)
- âœ… WebLLM provider (WebGPU)
- âœ… Mock provider for CI
- âœ… FSM with 12 states
- âœ… Epoch/Seq anti-race protocol
- âœ… Quota preflight check
- âœ… React hooks & Streaming
- âœ… Diagnostics API
- âœ… TypeScript strict mode

### Roadmap
- ðŸ”œ Real WASM model integration (Whisper, Piper TTS, Tesseract)
- ðŸ”œ WebGPU VLM model integration
- ðŸ”œ Function calling support
- ðŸ”œ Model fine-tuning in browser

## Contributing

Contributions are welcome! Please read our [Contributing Guide](./CONTRIBUTING.md) first.

### Quick Contribution Workflow

```bash
# 1. Fork and clone
git clone https://github.com/YOUR_USERNAME/browser-ai.git
cd browser-ai

# 2. Create a feature branch
git checkout -b feature/my-feature

# 3. Make changes and verify
pnpm run verify

# 4. Add a changeset (for version bump + changelog)
pnpm changeset

# 5. Commit and push
git add .
git commit -m "feat(core): add new feature"
git push origin feature/my-feature

# 6. Open a Pull Request
```

## License

MIT Â© 2026
